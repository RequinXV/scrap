import requests
import base64
import csv
import pandas as pd
from bs4 import BeautifulSoup
from Bass import Bass

class Scraper:
def init(self, base_url, uri, nb_pages=1):
self.base_url = base_url
self.uri = uri
self.nb_pages = nb_pages
self.urls = []
self.links = []
class Scraper:
def init(self, base_url, uri, nb_pages=1):
self.base_url = base_url
self.uri = uri
self.nb_pages = nb_pages
self.urls = []
self.links = []
def get_links(self):
    for i in range(self.nb_pages):
        self.urls.append(self.baseurl + self.uri + "?offset=" + str(i*20))
    for url in self.urls:
        response = requests.get(url)
        if response.ok:
            soup = BeautifulSoup(response.text, 'html.parser')
            lis = soup.findAll("li", class="playlist-item cards-item")
            for li in lis:
                try: 
                    href = li.find('a')['href']
                except TypeError:
                    href = base64.b64decode(li.span['data-submit']).decode('utf-8').replace("https://fr.audiofanzine.com/", "")
                self.links.append(self.base_url + href)
        else:
            print("ERROR: Failed Connect on :" + str(url))
    return self.links

def get_info_bypage(self, url):
    response = requests.get(url)
    if response.ok:
        soup = BeautifulSoup(response.text, 'html.parser')
        ul = soup.find("ul", class="product-information-header")
        spanstab = []
        if ul is not None:
            for uls in ul.find_all("li"):
                spans = uls.find_all("span")
                spanstab.append(spans)
            Fabricant = spanstab[0][1]
            Modele = spanstab[1][1]
            Serie = spanstab[2][1]
            Categorie = spanstab[3][1]
            fiche = {
                    "Fabricant": Fabricant,
                    "Modèle": Modele,
                    "Série": Serie,
                    "Categorie": Categorie,
                }
            return fiche
    else:
        print("ERROR: Failed Connect on :" + str(url))

def write_links_to_file(self, filename):
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerows([[link] for link in self.links])
def write_data_to_csv(self, filename):
    data = []
    for url in self.links:
        fiche = self.get_info_by_page(url)
        if fiche:
            data.append(fiche)
    fieldnames = ["Fabricant", "Modèle", "Série", "Catégorie"]
    with open(filename, 'w', encoding='UTF8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(data)

def run(self, links_filename, data_filename):
    self.get_links()
    self.write_links_to_file(links_filename)
    self.write_data_to_csv(data_filename)
    print("Done")